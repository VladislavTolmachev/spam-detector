{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39021256-ecad-452a-9752-c6d810fa677c",
   "metadata": {},
   "source": [
    "*SPAM FILTER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d623869e-0700-434e-891e-dc1a8e34b264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите пароль администратора для обучения модели:  ADMIN_SPAM_2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск процесса обучения модели...\n",
      "Загружено образцов: 5572\n",
      "Распределение классов: {'ham': 4825, 'spam': 747}\n",
      "Удалено дубликатов: 403\n",
      "Training set size: 4135\n",
      "Test set size: 1034\n",
      "Training base model...\n",
      "Base model performance:\n",
      "  F1-Score: 0.928\n",
      "  Precision: 0.918\n",
      "  Recall: 0.939\n",
      "  Average Precision: 0.944\n",
      "Applying probability calibration...\n",
      "\n",
      "============================================================\n",
      "FINAL TRAINING RESULTS\n",
      "============================================================\n",
      "CALIBRATED MODEL PREDICTIONS (threshold = 0.5):\n",
      "  F1-Score: 0.938\n",
      "  Precision: 0.953\n",
      "  Recall: 0.924\n",
      "\n",
      "OPTIMIZED PREDICTIONS FOR PRECISION AT 80% RECALL:\n",
      "  Optimal threshold: 0.883\n",
      "  F1-Score: 0.932\n",
      "  Precision: 0.975\n",
      "  Recall: 0.893\n",
      "  Precision at Recall 0.8: 0.975\n",
      "\n",
      "OVERALL METRICS:\n",
      "  Average Precision: 0.955\n",
      "  Precision at Top-100: 0.970\n",
      "\n",
      "ACCEPTANCE CRITERIA RESULTS:\n",
      "  AP >= 0.95: PASS\n",
      "  F1 >= 0.90: PASS\n",
      "  F1 lift >= 0.30: PASS\n",
      "  Precision at recall 0.8 >= 0.9: PASS\n",
      "\n",
      "✅ All acceptance criteria satisfied - Model is production ready!\n",
      "\n",
      "Model successfully saved: spam_classifier_final.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ВАШ ОРИГИНАЛЬНЫЙ КОД ОБУЧЕНИЯ МОДЕЛИ БЕЗ ИЗМЕНЕНИЙ\n",
    "\"\"\"\n",
    "\n",
    "import joblib\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class AdvancedTextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.num_pattern = re.compile(r'\\d+')\n",
    "        self.url_pattern = re.compile(r'http\\S+|www\\.\\S+')\n",
    "        self.email_pattern = re.compile(r'\\S+@\\S+')\n",
    "        self.special_char_pattern = re.compile(r'[^\\w\\s<>]')\n",
    "        self.extra_spaces = re.compile(r'\\s+')\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return [self._clean_text(text) for text in X]\n",
    "    \n",
    "    def _clean_text(self, message):\n",
    "        if not isinstance(message, str):\n",
    "            return \"\"\n",
    "        message = message.lower()\n",
    "        message = self.num_pattern.sub(' <NUM> ', message)\n",
    "        message = self.url_pattern.sub(' <URL> ', message)\n",
    "        message = self.email_pattern.sub(' <EMAIL> ', message)\n",
    "        message = self.special_char_pattern.sub(' ', message)\n",
    "        message = self.extra_spaces.sub(' ', message)\n",
    "        words = message.split()\n",
    "        filtered_words = [word for word in words if len(word) > 1]\n",
    "        return ' '.join(filtered_words).strip()\n",
    "\n",
    "\n",
    "class RegexBaseline:\n",
    "    def __init__(self):\n",
    "        self.spam_keywords = [\n",
    "            'free', 'win', 'prize', 'cash', 'urgent', 'call now',\n",
    "            'congratulations', 'selected', 'award', 'claim', 'limited',\n",
    "            'offer', 'guaranteed', 'txt', 'mobile', 'reply', 'stop',\n",
    "            'service', 'customer'\n",
    "        ]\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        return [\n",
    "            'spam' if any(kw in str(text).lower() for kw in self.spam_keywords)\n",
    "            else 'ham' for text in texts\n",
    "        ]\n",
    "    \n",
    "    def predict_proba(self, texts):\n",
    "        predictions = self.predict(texts)\n",
    "        probas = []\n",
    "        for pred in predictions:\n",
    "            if pred == 'spam':\n",
    "                probas.append([0.2, 0.8])\n",
    "            else:\n",
    "                probas.append([0.8, 0.2])\n",
    "        return np.array(probas)\n",
    "\n",
    "\n",
    "def calculate_precision_at_recall(y_true, y_proba, target_recall, pos_label='spam'):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba, pos_label=pos_label)\n",
    "    \n",
    "    idx = np.argmin(np.abs(recalls - target_recall))\n",
    "    precision_at_target = precisions[idx]\n",
    "    \n",
    "    return precision_at_target, thresholds[idx] if idx < len(thresholds) else 0.5\n",
    "\n",
    "\n",
    "def find_optimal_threshold_for_recall(y_true, y_proba, target_recall, pos_label='spam'):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba, pos_label=pos_label)\n",
    "    \n",
    "    valid_indices = np.where(recalls >= target_recall)[0]\n",
    "    if len(valid_indices) > 0:\n",
    "        best_idx = valid_indices[np.argmax(precisions[valid_indices])]\n",
    "        optimal_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "        optimal_precision = precisions[best_idx]\n",
    "    else:\n",
    "        best_idx = np.argmax(recalls)\n",
    "        optimal_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "        optimal_precision = precisions[best_idx]\n",
    "    \n",
    "    return optimal_threshold, optimal_precision\n",
    "\n",
    "\n",
    "def plot_precision_recall_tradeoff(y_test, y_proba, target_recall=0.8):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba, pos_label='spam')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    ax1.plot(recalls, precisions, linewidth=2, color='darkblue')\n",
    "    ax1.set_xlabel('Recall')\n",
    "    ax1.set_ylabel('Precision')\n",
    "    ax1.set_title('Precision-Recall Curve')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    target_idx = np.argmin(np.abs(recalls - target_recall))\n",
    "    ax1.axvline(x=target_recall, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Target Recall: {target_recall}')\n",
    "    ax1.scatter(recalls[target_idx], precisions[target_idx], color='red', s=100, \n",
    "                zorder=5, label=f'Precision: {precisions[target_idx]:.3f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(thresholds, precisions[:-1], label='Precision', linewidth=2)\n",
    "    ax2.plot(thresholds, recalls[:-1], label='Recall', linewidth=2)\n",
    "    ax2.set_xlabel('Classification Threshold')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Precision and Recall vs Threshold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    optimal_threshold, optimal_precision = find_optimal_threshold_for_recall(y_test, y_proba, target_recall)\n",
    "    ax2.axvline(x=optimal_threshold, color='red', linestyle='--', \n",
    "                label=f'Optimal threshold: {optimal_threshold:.3f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return optimal_threshold, optimal_precision\n",
    "\n",
    "\n",
    "def plot_performance_comparison(baseline_results, final_metrics):\n",
    "    models = list(baseline_results.keys()) + ['Final Model']\n",
    "    f1_scores = [baseline_results[model]['f1'] for model in baseline_results.keys()] + [final_metrics['f1']]\n",
    "    ap_scores = [baseline_results[model]['ap'] for model in baseline_results.keys()] + [final_metrics['ap']]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    x_pos = np.arange(len(models))\n",
    "    bars1 = ax1.bar(x_pos, f1_scores, color=['lightgray', 'lightgray', 'lightgray', 'steelblue'])\n",
    "    ax1.set_xlabel('Models')\n",
    "    ax1.set_ylabel('F1-Score')\n",
    "    ax1.set_title('Model Comparison: F1-Score')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(models, rotation=45)\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, bar in enumerate(bars1):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{f1_scores[i]:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    bars2 = ax2.bar(x_pos, ap_scores, color=['lightgray', 'lightgray', 'lightgray', 'darkorange'])\n",
    "    ax2.set_xlabel('Models')\n",
    "    ax2.set_ylabel('Average Precision')\n",
    "    ax2.set_title('Model Comparison: Average Precision')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(models, rotation=45)\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, bar in enumerate(bars2):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{ap_scores[i]:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_importance_analysis(feature_names, coefficients, top_n=20):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coefficients\n",
    "    })\n",
    "    \n",
    "    top_spam = feature_importance.nlargest(top_n, 'coefficient')\n",
    "    top_ham = feature_importance.nsmallest(top_n, 'coefficient')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))\n",
    "    \n",
    "    y_pos = np.arange(len(top_spam))\n",
    "    ax1.barh(y_pos, top_spam['coefficient'], color='firebrick', alpha=0.7)\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(top_spam['feature'])\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('Coefficient Value')\n",
    "    ax1.set_title(f'Top {top_n} Spam Indicators')\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    for i, v in enumerate(top_spam['coefficient']):\n",
    "        ax1.text(v + 0.01, i, f'{v:.2f}', va='center', fontsize=9)\n",
    "    \n",
    "    y_pos = np.arange(len(top_ham))\n",
    "    ax2.barh(y_pos, top_ham['coefficient'], color='steelblue', alpha=0.7)\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(top_ham['feature'])\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.set_xlabel('Coefficient Value')\n",
    "    ax2.set_title(f'Top {top_n} Ham Indicators')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    for i, v in enumerate(top_ham['coefficient']):\n",
    "        ax2.text(v - 0.15, i, f'{v:.2f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_comprehensive_metrics(y_test, y_pred, y_proba, model_name):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    y_test_binary = np.where(y_test == 'spam', 1, 0)\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_pred, normalize='true', \n",
    "        display_labels=['Ham', 'Spam'], ax=axes[0, 0], cmap='Blues'\n",
    "    )\n",
    "    axes[0, 0].set_title('Confusion Matrix')\n",
    "    \n",
    "    precisions, recalls, _ = precision_recall_curve(y_test_binary, y_proba)\n",
    "    ap_score = average_precision_score(y_test_binary, y_proba)\n",
    "    axes[0, 1].plot(recalls, precisions, linewidth=2, color='darkblue')\n",
    "    axes[0, 1].set_xlabel('Recall')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].set_title(f'Precision-Recall Curve (AP = {ap_score:.3f})')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test_binary, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    axes[1, 0].plot(fpr, tpr, linewidth=2, color='darkred')\n",
    "    axes[1, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[1, 0].set_xlabel('False Positive Rate')\n",
    "    axes[1, 0].set_ylabel('True Positive Rate')\n",
    "    axes[1, 0].set_title(f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    spam_probs = y_proba[y_test == 'spam']\n",
    "    ham_probs = y_proba[y_test == 'ham']\n",
    "    \n",
    "    axes[1, 1].hist(ham_probs, bins=30, alpha=0.7, label='Ham', color='blue', density=True)\n",
    "    axes[1, 1].hist(spam_probs, bins=30, alpha=0.7, label='Spam', color='red', density=True)\n",
    "    axes[1, 1].set_xlabel('Predicted Probability (Spam)')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].set_title('Probability Distributions by Class')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SMS SPAM CLASSIFIER\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    data_file = 'SMSSpamCollection'\n",
    "    try:\n",
    "        df = pd.read_csv(data_file, sep='\\t', header=None,\n",
    "                        names=['label', 'message'], encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(data_file, sep='\\t', header=None,\n",
    "                        names=['label', 'message'], encoding='latin-1')\n",
    "    \n",
    "    print(f\"Dataset loaded: {len(df)} total samples\")\n",
    "    print(f\"Class distribution: {df['label'].value_counts().to_dict()}\")\n",
    "    \n",
    "    spam_prevalence = len(df[df['label'] == 'spam']) / len(df)\n",
    "    print(f\"Spam prevalence: {spam_prevalence:.3f}\")\n",
    "    \n",
    "    initial_size = len(df)\n",
    "    df = df.drop_duplicates(subset=['message'])\n",
    "    final_size = len(df)\n",
    "    print(f\"Duplicate messages removed: {initial_size - final_size}\")\n",
    "    \n",
    "    X = df['message']\n",
    "    y = df['label']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BASELINE MODEL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    baselines = {\n",
    "        'most_frequent': DummyClassifier(strategy='most_frequent'),\n",
    "        'stratified': DummyClassifier(strategy='stratified', random_state=42),\n",
    "        'regex': RegexBaseline()\n",
    "    }\n",
    "    \n",
    "    baseline_results = {}\n",
    "    for name, baseline in baselines.items():\n",
    "        if hasattr(baseline, 'fit'):\n",
    "            baseline.fit(X_train, y_train)\n",
    "            y_pred = baseline.predict(X_test)\n",
    "            if hasattr(baseline, 'predict_proba'):\n",
    "                y_proba = baseline.predict_proba(X_test)\n",
    "                if len(y_proba.shape) > 1 and y_proba.shape[1] > 1:\n",
    "                    y_proba = y_proba[:, 1]\n",
    "                else:\n",
    "                    y_proba = y_proba\n",
    "            else:\n",
    "                y_proba = np.array([1.0 if pred == 'spam' else 0.0 for pred in y_pred])\n",
    "        else:\n",
    "            y_pred = baseline.predict(X_test)\n",
    "            y_proba = baseline.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
    "        ap = average_precision_score(y_test, y_proba, pos_label='spam')\n",
    "        baseline_results[name] = {'f1': f1, 'ap': ap}\n",
    "        print(f\"{name:15} F1-score: {f1:.3f}, Average Precision: {ap:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MAIN MODEL TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    \n",
    "    base_pipeline = Pipeline([\n",
    "        ('clean', AdvancedTextCleaner()),\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=8000,\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            analyzer='word',\n",
    "            sublinear_tf=True\n",
    "        )),\n",
    "        ('model', LogisticRegression(\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            max_iter=2000,\n",
    "            C=1.0,\n",
    "            solver='liblinear'\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    print(\"Training base model...\")\n",
    "    base_pipeline.fit(X_train, y_train_encoded)\n",
    "    \n",
    "    y_proba_base = base_pipeline.predict_proba(X_test)[:, 1]\n",
    "    y_pred_base = base_pipeline.predict(X_test)\n",
    "    y_pred_base_labels = label_encoder.inverse_transform(y_pred_base)\n",
    "    \n",
    "    precision_base = precision_score(y_test, y_pred_base_labels, pos_label='spam', zero_division=0)\n",
    "    recall_base = recall_score(y_test, y_pred_base_labels, pos_label='spam')\n",
    "    f1_base = f1_score(y_test, y_pred_base_labels, pos_label='spam')\n",
    "    ap_base = average_precision_score(y_test, y_proba_base, pos_label='spam')\n",
    "    \n",
    "    print(f\"Base model performance:\")\n",
    "    print(f\"  F1-Score: {f1_base:.3f}\")\n",
    "    print(f\"  Precision: {precision_base:.3f}\")\n",
    "    print(f\"  Recall: {recall_base:.3f}\")\n",
    "    print(f\"  Average Precision: {ap_base:.3f}\")\n",
    "    \n",
    "    print(\"Applying probability calibration...\")\n",
    "    calibrated_model = CalibratedClassifierCV(\n",
    "        base_pipeline.named_steps['model'],\n",
    "        method='isotonic',\n",
    "        cv=3\n",
    "    )\n",
    "    \n",
    "    calibrated_pipeline = Pipeline([\n",
    "        ('clean', base_pipeline.named_steps['clean']),\n",
    "        ('tfidf', base_pipeline.named_steps['tfidf']),\n",
    "        ('model', calibrated_model)\n",
    "    ])\n",
    "    \n",
    "    calibrated_pipeline.fit(X_train, y_train_encoded)\n",
    "    final_pipeline = calibrated_pipeline\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    y_proba_calibrated = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "    y_pred_calibrated_encoded = final_pipeline.predict(X_test)\n",
    "    y_pred_calibrated = label_encoder.inverse_transform(y_pred_calibrated_encoded)\n",
    "    \n",
    "    target_recall = 0.80\n",
    "    optimal_threshold, precision_at_target = find_optimal_threshold_for_recall(\n",
    "        y_test, y_proba_calibrated, target_recall, 'spam'\n",
    "    )\n",
    "    \n",
    "    y_pred_optimized = np.where(y_proba_calibrated >= optimal_threshold, 'spam', 'ham')\n",
    "    \n",
    "    accuracy_cal = accuracy_score(y_test, y_pred_calibrated)\n",
    "    precision_cal = precision_score(y_test, y_pred_calibrated, pos_label='spam', zero_division=0)\n",
    "    recall_cal = recall_score(y_test, y_pred_calibrated, pos_label='spam')\n",
    "    f1_cal = f1_score(y_test, y_pred_calibrated, pos_label='spam')\n",
    "    \n",
    "    accuracy_opt = accuracy_score(y_test, y_pred_optimized)\n",
    "    precision_opt = precision_score(y_test, y_pred_optimized, pos_label='spam', zero_division=0)\n",
    "    recall_opt = recall_score(y_test, y_pred_optimized, pos_label='spam')\n",
    "    f1_opt = f1_score(y_test, y_pred_optimized, pos_label='spam')\n",
    "    \n",
    "    ap_cal = average_precision_score(y_test, y_proba_calibrated, pos_label='spam')\n",
    "    \n",
    "    k = 100\n",
    "    indices_top_k = np.argsort(y_proba_calibrated)[-k:]\n",
    "    precision_at_k = sum(1 for i in indices_top_k if y_test.iloc[i] == 'spam') / k\n",
    "    \n",
    "    print(\"CALIBRATED MODEL PREDICTIONS (threshold = 0.5):\")\n",
    "    print(f\"  F1-Score: {f1_cal:.3f}\")\n",
    "    print(f\"  Precision: {precision_cal:.3f}\")\n",
    "    print(f\"  Recall: {recall_cal:.3f}\")\n",
    "    print(f\"  Accuracy: {accuracy_cal:.3f}\")\n",
    "    \n",
    "    print(\"\\nOPTIMIZED PREDICTIONS FOR PRECISION AT 80% RECALL:\")\n",
    "    print(f\"  Optimal threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"  F1-Score: {f1_opt:.3f}\")\n",
    "    print(f\"  Precision: {precision_opt:.3f}\")\n",
    "    print(f\"  Recall: {recall_opt:.3f}\")\n",
    "    print(f\"  Accuracy: {accuracy_opt:.3f}\")\n",
    "    print(f\"  Precision at Recall {target_recall}: {precision_at_target:.3f}\")\n",
    "    \n",
    "    print(f\"\\nOVERALL METRICS:\")\n",
    "    print(f\"  Average Precision: {ap_cal:.3f}\")\n",
    "    print(f\"  Precision at Top-{k}: {precision_at_k:.3f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report (Optimized Threshold):\")\n",
    "    print(classification_report(y_test, y_pred_optimized, target_names=['Ham', 'Spam']))\n",
    "    \n",
    "    print(\"\\nGenerating precision-recall tradeoff analysis...\")\n",
    "    plot_precision_recall_tradeoff(y_test, y_proba_calibrated, target_recall)\n",
    "    \n",
    "    print(\"\\nGenerating comprehensive performance visualizations...\")\n",
    "    plot_comprehensive_metrics(y_test, y_pred_optimized, y_proba_calibrated, \"Final Model\")\n",
    "    \n",
    "    print(\"\\nGenerating model comparison visualization...\")\n",
    "    final_metrics = {'f1': f1_opt, 'ap': ap_cal}\n",
    "    plot_performance_comparison(baseline_results, final_metrics)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ACCEPTANCE CRITERIA ASSESSMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    ap_required = 0.95\n",
    "    f1_required = 0.90\n",
    "    precision_required = 0.90\n",
    "    \n",
    "    baseline_f1 = baseline_results['most_frequent']['f1']\n",
    "    f1_lift = f1_opt - baseline_f1\n",
    "    \n",
    "    criteria_met = {\n",
    "        'AP >= 0.95': ap_cal >= ap_required,\n",
    "        'F1 >= 0.90': f1_opt >= f1_required,\n",
    "        'F1 lift >= 0.30': f1_lift >= 0.30,\n",
    "        f'Precision at recall {target_recall} >= {precision_required}': precision_at_target >= precision_required\n",
    "    }\n",
    "    \n",
    "    print(\"Acceptance Criteria Results (using optimized threshold):\")\n",
    "    for criterion, met in criteria_met.items():\n",
    "        status = \"PASS\" if met else \"FAIL\"\n",
    "        print(f\"  {criterion}: {status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FEATURE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    feature_names = base_pipeline.named_steps['tfidf'].get_feature_names_out()\n",
    "    coefficients = base_pipeline.named_steps['model'].coef_[0]\n",
    "    \n",
    "    top_spam = np.argsort(coefficients)[-15:][::-1]\n",
    "    top_ham = np.argsort(coefficients)[:15]\n",
    "    \n",
    "    print(\"Top 15 SPAM indicators:\")\n",
    "    for idx in top_spam:\n",
    "        print(f\"  {feature_names[idx]}: {coefficients[idx]:.3f}\")\n",
    "        \n",
    "    print(\"\\nTop 15 HAM indicators:\")\n",
    "    for idx in top_ham:\n",
    "        print(f\"  {feature_names[idx]}: {coefficients[idx]:.3f}\")\n",
    "    \n",
    "    print(\"\\nGenerating feature importance visualization...\")\n",
    "    plot_feature_importance_analysis(feature_names, coefficients)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL IMPLEMENTATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"Optimization Strategies Applied:\")\n",
    "    strategies = [\n",
    "        \"Enhanced feature engineering with increased vocabulary\",\n",
    "        \"Sublinear TF scaling for better term weighting\", \n",
    "        \"Isotonic calibration for improved probability estimates\",\n",
    "        \"Optimal threshold selection for target recall\",\n",
    "        \"Label encoding for proper class weight handling\"\n",
    "    ]\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"  - {strategy}\")\n",
    "    \n",
    "    print(f\"\\nFinal Performance with Optimized Threshold ({optimal_threshold:.3f}):\")\n",
    "    print(f\"  Average Precision: {ap_cal:.3f} (Target: >= 0.95)\")\n",
    "    print(f\"  F1-Score: {f1_opt:.3f} (Target: >= 0.90)\")\n",
    "    print(f\"  F1 Improvement over baseline: {f1_lift:.3f} (Target: >= 0.30)\")\n",
    "    print(f\"  Precision at recall {target_recall}: {precision_at_target:.3f} (Target: >= 0.90)\")\n",
    "    \n",
    "    all_criteria_met = all(criteria_met.values())\n",
    "    if all_criteria_met:\n",
    "        print(\"\\nResult: All acceptance criteria satisfied\")\n",
    "        print(\"Status: Production ready\")\n",
    "    else:\n",
    "        print(\"\\nResult: Some acceptance criteria not satisfied\")\n",
    "        if not criteria_met[f'Precision at recall {target_recall} >= {precision_required}']:\n",
    "            print(\"Recommendation: The model achieves high recall but precision at 80% recall needs improvement\")\n",
    "            print(\"Consider: Additional feature engineering, ensemble methods, or domain-specific rules\")\n",
    "    \n",
    "    model_artifact = {\n",
    "        'production_model': final_pipeline,\n",
    "        'base_model': base_pipeline,\n",
    "        'label_encoder': label_encoder,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'metadata': {\n",
    "            'performance': {\n",
    "                'f1_score_optimized': f1_opt,\n",
    "                'f1_score_calibrated': f1_cal,\n",
    "                'f1_score_base': f1_base,\n",
    "                'average_precision': ap_cal,\n",
    "                'precision_optimized': precision_opt,\n",
    "                'recall_optimized': recall_opt,\n",
    "                'precision_calibrated': precision_cal,\n",
    "                'recall_calibrated': recall_cal,\n",
    "                'precision_at_recall_80': precision_at_target,\n",
    "                'precision_at_top_100': precision_at_k,\n",
    "                'f1_lift_vs_baseline': f1_lift\n",
    "            },\n",
    "            'acceptance_criteria': {\n",
    "                'ap_met': ap_cal >= ap_required,\n",
    "                'f1_met': f1_opt >= f1_required,\n",
    "                'f1_lift_met': f1_lift >= 0.30,\n",
    "                'precision_recall_met': precision_at_target >= precision_required,\n",
    "                'all_criteria_met': all_criteria_met\n",
    "            },\n",
    "            'dataset_info': {\n",
    "                'total_samples': len(df),\n",
    "                'train_samples': len(X_train),\n",
    "                'test_samples': len(X_test),\n",
    "                'spam_prevalence': float(spam_prevalence)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_artifact, 'spam_classifier_final.pkl')\n",
    "    print(f\"\\nModel artifact saved: spam_classifier_final.pkl\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd8f81-6590-48f7-9067-fd021bc01edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d057d10e-da4c-4486-999e-52777eed17a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76448c55-ba86-4fc1-bc49-94df2599896a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9bcf4f-7b11-45f0-b85b-db8340d2a874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
